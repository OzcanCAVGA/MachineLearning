# 1Regresyon

**Bagimli Degisken(Y):** bir olay veya deneyin ciktisini temsil eder. Bagimli degiskenimiz, deneydeki diger degiskenlerin hareketliliginden etkilenir. **y** ile sembolize ediyoruz.

Tahmin edilmek istenen deger 

**Bagimsiz Degisken(X):** Bagimli degisken uzerinde etki yaratan degiskenlere denir. **X** ile gosteriyoruz. 

## Regresyon Tanimi

Bagimli degiskenin bagimsiz degiskenlerle olan iliskini modellemek icin kullanilan istatisksel bir yontemdir.

Regresyon Modelleri:

- **DoÄŸrusal Regresyon: BaÄŸÄ±mlÄ± deÄŸiÅŸken ile baÄŸÄ±msÄ±z deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkiyi doÄŸrusal bir denklemle ifade eder.**
- Polinom Regresyon: BaÄŸÄ±mlÄ± deÄŸiÅŸken ile baÄŸÄ±msÄ±z deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkiyi polinom bir denklemle ifade eder.
- Lojistik Regresyon: BaÄŸÄ±mlÄ± deÄŸiÅŸkenin iki kategorik sÄ±nÄ±f arasÄ±ndaki iliÅŸkisini modellemek iÃ§in kullanÄ±lÄ±r.

Regresyon Analizinin Uygulama AlanlarÄ±:

- Pazarlama: ÃœrÃ¼n fiyatlarÄ±nÄ±n talep Ã¼zerindeki etkisini tahmin etmek.
- Ekonomi: Ekonomik deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkileri analiz etmek.
- SaÄŸlÄ±k: HastalÄ±k riskini tahmin etmek veya tedavi sonuÃ§larÄ±nÄ± deÄŸerlendirmek.
- Finans: Hisse senedi fiyatlarÄ±nÄ± tahmin etmek veya risk analizi yapmak.

```python
house = pd.read_csv("../resources/housing.csv")
house

# Konut fiyatlarini etkileyen cesitli faktorleri incelemek 
# icin olusturulmus bir dataframe
```

![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled.png)

```python
house.info()
# **total_bedrooms** inceledigimizde, 207 tane bos veri var. 
# Bunlar analiz icin sorun cikartir.
```

![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%201.png)

Bakildiginda, elimizdeki dataframeâ€™de eksik veriler var. Bunun icin birkac yontem var.

1. **Eksik Verileri Silme:**
    - EÄŸer eksik verilerin olduÄŸu satÄ±rlarÄ±n sayÄ±sÄ± Ã§ok az ise, bu satÄ±rlarÄ± veri setinden tamamen Ã§Ä±karabilirsiniz. Ancak, veri setinizdeki Ã¶nemli bilgileri kaybetme riski olduÄŸunu unutmayÄ±n
2. **Eksik Verileri Doldurma:**
- Eksik verileri, o kolondaki diÄŸer verilerin istatistiksel Ã¶zelliklerini kullanarak doldurabilirsiniz. Ã–rneÄŸin, eksik verileri o kolondaki diÄŸer verilerin ortalamasÄ±, medyanÄ± veya modu ile doldurabilirsiniz.
- Eksik verileri doldurmak iÃ§in pandas kÃ¼tÃ¼phanesindeki fillna() fonksiyonunu kullanabilirsiniz. Ã–rneÄŸin, eksik verileri ortalamayla doldurmak iÃ§in aÅŸaÄŸÄ±daki gibi bir kod kullanabilirsiniz:

```python
df['eksik_kolon'].fillna(df['eksik_kolon'].mean(), inplace=True)
```

1. Eksik Verileri Tahmin Etme:
    - Eksik verileri, diÄŸer baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin kullanÄ±ldÄ±ÄŸÄ± bir tahmin modeli kullanarak tahmin edebilirsiniz. Ã–rneÄŸin, eksik verileri bir doÄŸrusal regresyon modeli kullanarak tahmin edebilirsiniz.
    - Eksik verileri tahmin etmek iÃ§in sklearn kÃ¼tÃ¼phanesindeki LinearRegression() sÄ±nÄ±fÄ±nÄ± kullanabilirsiniz. Ã–rneÄŸin, eksik verileri doÄŸrusal regresyon ile tahmin etmek iÃ§in aÅŸaÄŸÄ±daki gibi bir kod kullanabilirsiniz:
    
    ---
    
    Biz ise inceledigimiz dataframeâ€™de once ortalama ve medyan degerlerine bakalim.
    
    ```python
    from statistics import median
    print(f"Ortalama = {house.total_bedrooms.mean()}")
    print(f"Medyan = {median(house.total_bedrooms)}")
    
    #Bu kod ile total_bedrooms icin **ortalama'sini** ve **Medyan** degerinin hesaplanmasini gosteriyor.
    #Eger kullanmaya elverisli olup olmadigini gormek istersek buradan hesapliyoruz.
    
    # Cikti =>
    Ortalama = 537.8705525375618
    Medyan = 429.0
    ```
    
    ```python
    import seaborn as sns
    sns.histplot(house.total_bedrooms, kde=True)
    # kde = True -> yogunluk cizgisi olarak adlandiririz.
    # elimizdeki verileri gorsellestiriyoruz.
    """
    Seaborn kutuphanesini import edip **total_bedrooms** icin 
    histogram cizdiriyoruz.
    histogram cizmek icin **histplot** fonksiyonu kullaniriz
    
    **kde=True** => yogunluk egrisi
    
    """
    ```
    
    ```python
    house.dropna(inplace=True) 
    # eksik degerlere sahip satirlari siliyoruz ve kaydediyoruz.
    
    house.info() 
    # total_bedrooms'taki eksik degerli olan satirlari sildik ve tum dataframe'de 
    # eksik satir kalmadi.
    
    # eksik satir problemi icin 3 farkli yol var.
    # 1. Medyan ile doldur
    # 2. Ortalama ile doldur
    # 3. Dropna ile satirlari sil.
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%202.png)
    
    ```python
    sns.histplot(house.total_bedrooms, kde=True)
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%203.png)
    
    ### skew() fonksiyonu â†’ Carpikligi anlama
    
    ```python
    from scipy.stats import skew
    ```
    
    **scipy.stats** kutuphanesinden **skew()** fonksiyonunu import ettik.
    ***total_bedrooms*** sutunun *carpikligini(skewness)* degerini hesaplamak icin kullanacagiz.
    
    Bir veri dagiliminin simetrisizligini veya simetrik olma durumunu gormek icin hesaplanir.
    
    > **Negatif bir deÄŸer**: Veri setinin sol tarafta uzandÄ±ÄŸÄ±nÄ± (saÄŸa Ã§arpÄ±k olduÄŸunu) gÃ¶sterir.
    **Pozitif bir deÄŸer:** Veri setinin saÄŸ tarafta uzandÄ±ÄŸÄ±nÄ± (sola Ã§arpÄ±k olduÄŸunu) gÃ¶sterir.
    SÄ±fÄ±ra yakÄ±n bir deÄŸer: DaÄŸÄ±lÄ±mÄ±n daha simetrik olduÄŸunu ifade eder.
    
    0'a YakÄ±n DeÄŸerler: DaÄŸÄ±lÄ±mÄ±n simetrik olduÄŸunu veya Ã§arpÄ±klÄ±ÄŸÄ±n Ã§ok az olduÄŸunu gÃ¶sterir.
    0 ile Â±0.5 ArasÄ± DeÄŸerler: Ã‡ok hafif bir Ã§arpÄ±klÄ±ÄŸÄ± gÃ¶sterir.
    Â±0.5 ile Â±1 ArasÄ± DeÄŸerler: Orta dÃ¼zeyde Ã§arpÄ±klÄ±k olduÄŸunu ifade eder.
    Â±1'den BÃ¼yÃ¼k DeÄŸerler: YÃ¼ksek Ã§arpÄ±klÄ±ÄŸÄ± belirtir
    > 
    
    ```python
    import numpy as np
    from scipy.stats import skew
    
    # SayÄ± dizisi
    data = np.array([1, 1, 1, 2, 3, 4, 4, 5, 18, 190])
    
    # Ã‡arpÄ±klÄ±k hesaplamasÄ±
    skewness = skew(data)
    
    # Sonucu yazdÄ±rma
    print("Ã‡arpÄ±klÄ±k:", skewness)
    
    # Cikti
    # Ã‡arpÄ±klÄ±k: 2.6316589096189
    ```
    
    Negatif Carpiklik:
    
    ```python
    import numpy as np
    from scipy.stats import skew
    
    # Negatif Ã§arpÄ±klÄ±ÄŸa sahip veri seti
    negative_skewed_data = np.array([1000, 900, 800, 700, 600, 500, 400, 300, 20, 10])
    
    #baska bir veri ornegi
    # negative_skewed_data = np.array([1, 2000, 3000, 4000, 5000, 6000, 7000])
    skewness = skew(negative_skewed_data)
    
    # Sonucu yazdÄ±rma
    print("Ã‡arpÄ±klÄ±k:", skewness)
    
    # Ã‡arpÄ±klÄ±k: -0.24697270438946498
    ```
    
    Verisetimiz icin konusacak olursakta eger:
    
    ```python
    skew(house.total_bedrooms)
    #cikti: **3.45929235876747**
    ```
    
    > *total_bedrooms* sutunun carpiklik degeri pozitif olarak **3.45**. Bu bize **saga dogru carpiklasmanin** oldugunu soyluyor. Yani ortalamamiz normalde **daha dusuk iken** ortalamayi yukseltip **yukari cikartan birkac deger**
     var. Bunun yuksek olmasi, genel verilerimiz icin ortalama degerin dogru
     bir yorumlamaya acik olmadigini gosterir. Yani 10 tane verimiz var 
    elimizde. Bunlardan **8 tanesi cok dusuk**, eger bunlara 
    gore ortalama bir deger hesabi yaparsak ortalamamiz dusuk cikar. Ama 
    kalan 2 deger o kadar yuksek ki, ortalama degerimizi yukari yonlu bir 
    degere cikartiyor. Bu da genel verilerimiz icin ortalama degerin dogru **bir veri olmadigini soylemis** oluyor.
    > 
    
    ### Korelasyon
    
    > **Korelasyon**, iki deÄŸiÅŸken arasÄ±ndaki iliÅŸkiyi Ã¶lÃ§en istatistiksel bir kavramdÄ±r. Korelasyon, deÄŸiÅŸkenlerin birlikte nasÄ±l hareket ettiÄŸini ve birbirlerine ne kadar baÄŸÄ±mlÄ± olduklarÄ±nÄ± gÃ¶sterir.
    Korelasyon katsayÄ±sÄ±, **-1** ile **1** arasÄ±nda deÄŸer alÄ±r. **Pozitif bir korelasyon** katsayÄ±sÄ±, iki deÄŸiÅŸken arasÄ±nda doÄŸrusal bir iliÅŸki olduÄŸunu gÃ¶sterirken, **negatif bir korelasyon** katsayÄ±sÄ± ise iki deÄŸiÅŸken arasÄ±nda ters yÃ¶nlÃ¼ bir iliÅŸki olduÄŸunu gÃ¶sterir. Korelasyon katsayÄ±sÄ± **0** ise, iki deÄŸiÅŸken arasÄ±nda bir iliÅŸki olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.
    > 
    
    ```python
    house.drop("ocean_proximity", axis=1).corr()
    ```
    
    > # **ocean_proximity** sutunu icin herhangi bir corelasyon yapamayacagimiz icin once onu kaldiriyoruz. 
    # Daha sonra .corr() fonksiyonunu cagiriyoruz ve her alanin kendi iclerindeki corelasyonunu hesapliyoruz.
    # korelasyon katsayilari +1 ile -1 arasinda degeri vardir. 
    # +1'e yakin olan bir degerde, iki degisken arasinda guclu bir korelasyonun oldugunu gosterir. 
    # Bir degiskenin degeri artarken, diger degiskende onunla beraber arttigini gosterir. Bu iyidir.
    
    # -1'e yaklasan negatif bir korelasyon ise iki degisken arasinda guclu bir bag olmadiigini, birisi artarken
    #digerinin de azaldigini gosterir. Guclu bir dogrusal iliski vardir.
    
    # 0'a yakin korelasyonda ise iki degisken arasindaki bagin zayif oldugunu gosterir.
    > 
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%204.png)
    
    > Ã–rneÄŸin, **median_house_value** (ortalama ev deÄŸeri) ile **median_income** (ortalama gelir) arasÄ±nda **0.688** olan **pozitif bir korelasyon** var. Bu deÄŸer, ortalama gelir ile ev deÄŸeri arasÄ±nda orta dÃ¼zeyde pozitif bir iliÅŸki olduÄŸunu gÃ¶sterir. Yani, genellikle **ortalama gelir arttÄ±kÃ§a ev deÄŸeri de artma** eÄŸilimindedir.
    > 
    
    > **longitude** ve **latitude** arasÄ±nda **-0.92** olan gÃ¼Ã§lÃ¼ **negatif bir korelasyon** vardÄ±r. Bu, coÄŸrafi olarak birbirine yakÄ±n bÃ¶lgelerin koordinatlarÄ± arasÄ±nda **gÃ¼Ã§lÃ¼ bir ters iliÅŸki** olduÄŸunu gÃ¶sterir. Yani, enlem ve boylam arasÄ±nda **gÃ¼Ã§lÃ¼ bir ters iliÅŸki** vardÄ±r, yani biri artarken diÄŸeri azalma eÄŸilimindedir.
    > 
    
    ### Korelasyon Haritasini cizmek istersekte eger:
    
    ```python
    sns.heatmap(house.drop("ocean_proximity", axis=1).corr(), annot=True)
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%205.png)
    
    COKLU BAGLANTI PROBLEMI VAR
    
    ## DataFrame Hazirlama
    
    Dataframeâ€™e baktigimiz zaman, kategorik degerleri sayisallastirirsak eger modelimizin anlayacagi bir sekile getirmis oluruz
    
    Bunun icin once **pd.get.dummies()** fonksiyonunu kullanacagiz. Bu fonksiyon elimizde bulunan kategorik sutunu, yeni bir dataframe olusturarak benzersiz degerler icin birer sutun olusturur. Her **id**â€™nin hangi degerdeyse, o degere **1** koyar, diger degerlere 0 koyar. Bu sayede, kategorik sutunumuz sayisallasmis olur.
    
    Bu dataframeâ€™de ise **ocean_proximity** sutununa bakiyoruz. Benzersiz degerleri
    
    `array(['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND']`
    
    bunlar.
    
    Bunlar icin once hepsine sutun olusturup; 1 ve 0 degerlerini yerlestiriyor. 
    
    ```python
    house_new = house[["median_income", "ocean_proximity", "median_house_value"]] 
    house_new
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%206.png)
    
    **ocean_proximity** sutunun benzersiz degerlerine bakiyoruz
    
    ```python
    house_new.ocean_proximity.unique()
    # array(['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'],
    #      dtype=object)
    ```
    
    ```python
    pd.get_dummies(house_new.ocean_proximity, dtype=int) 
    # ocean_proximity sutunu, metin tabanli bir sutun. Bunu sayisal forma donusturmek icin kullaniyoruz.
    ```
    
    olusturdugumuz yeni dataframeâ€™i sectikten sonra fonksiyonumuzu uyguluyoruz.
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%207.png)
    
    Simdi olusturdugumuz **house_new** adli yeni dataframeâ€™e kategoriden sayisal veri haline getirdgimiz yeni sutunlari ekleyelim.
    
    Daha sonra da kategori halinde olan **ocean_proximity** sutununu silelim.
    
    ```python
    house_new = house_new.join(pd.get_dummies(house_new.ocean_proximity, dtype=int))
    house_new.drop("ocean_proximity", axis=1, inplace=True)
    house_new
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%208.png)
    
    ---
    
    Simdi tahmin etmesini istedigimiz **median_house_value** sutununu dataframeâ€™den kaldiriyoruz. Kaldirdiktan sonra olusan yeni dataframeâ€™imiz **BAGIMSIZ DEGISKEN(X)**â€™leri iceriyor sadece.
    
    ```python
    X = house_new.drop("median_house_value", axis=1)
    X
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%209.png)
    
    > **Bagimli Degisken:**
     Bir deneyin sonucunu veya incelenen olayin ciktisini temsil eder. 
    Bagimli degisken, diger degiskenlerden etkilenir. Genellikle tahmin 
    etmeye calistigimiz veya anlamaya calistigimiz degiskendir.
    Dataframe'imizde ise **ev fiyatlari** bagimli degisken oluyor. Evin konumu, buyuklugu, oda sayilari gibi degiskenler bagimli degiskenimizi etkiliyor olabilir.
    > 
    > 
    > **Bagimsiz Degisken:** Bagimli degisken uzerinde etki 
    > yaratan degiskenlerdir. Bagimsiz degiskenler, bagimli degisken uzerinde 
    > etki yaratir. Bagimli degiskeni aciklamak veya tahmin etmek icin 
    > kullanilir.
    > 
    > **X = house_new.drop("median_house_value", axis=1)**
    > 
    > Bu satÄ±rda, house_new DataFrame'inden "median_house_value" sÃ¼tunu 
    > drop() fonksiyonuyla Ã§Ä±karÄ±lÄ±yor. Burada "median_house_value" baÄŸÄ±mlÄ± 
    > deÄŸiÅŸkeni temsil ediyor olabilir. ArdÄ±ndan, geriye kalan sÃ¼tunlar, yani 
    > "median_income", "<1H OCEAN", "INLAND", "ISLAND", "NEAR BAY", "NEAR 
    > OCEAN" baÄŸÄ±msÄ±z deÄŸiÅŸkenler olarak X'e atanÄ±yor. Yani X, baÄŸÄ±msÄ±z 
    > deÄŸiÅŸkenleri iÃ§eren bir DataFrame oluyor. Bu veriler, muhtemelen bir 
    > makine Ã¶ÄŸrenimi modelinde baÄŸÄ±mlÄ± deÄŸiÅŸkeni tahmin etmek iÃ§in 
    > kullanÄ±lacak Ã¶zelliklerdir.
    > 
    
    Simdi ise tahmin edilmesini istedigimiz sutunu alip, **y** degiskenine atiyoruz. Bu sayede bunu modele egitmek icin verecegiz. Egitim sonrasinda modelimiz tahminlerini yapacak.
    
    ```python
    y = house_new["median_house_value"]
    y
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%2010.png)
    
    > Bu kod, house_new DataFrame'indeki **"median_house_value"** sÃ¼tununu **baÄŸÄ±mlÄ± deÄŸiÅŸken olarak (yani hedef deÄŸiÅŸkeni olarak)** kullanmak iÃ§in ayrÄ±ÅŸtÄ±rÄ±yor.
    > 
    > 
    > Bu satÄ±r, *house_new* DataFrame'inde bulunan "median_house_value" sÃ¼tununu seÃ§iyor. Bu sÃ¼tun, muhtemelen **modelin Ã¶ÄŸrenmeye Ã§alÄ±ÅŸacaÄŸÄ± ve tahmin etmeye Ã§alÄ±ÅŸacaÄŸÄ± deÄŸer** olan **ev fiyatlarÄ±nÄ± (median_house_value)** iÃ§eriyor. **y** deÄŸiÅŸkenine bu sÃ¼tunun verileri atandÄ±.
    > 
    
    ### Model egitme
    
    Elimizdeki veri setinim **egitim** ve **test setlerine** ayirmak icin **train_test_split** fonksiyonunu kullanacagiz. 
    
    Verisetini ayirip, **egitim** setiyle modelimizi egitecegiz. Daha sonra daha once hic gormedigi veriler olan **test setleri** ile de modelimizi deneyecegiz.
    
    ```python
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                        test_size=0.20, random_state=42)
    ```
    
    Egitimde kullanilacak degiskenlerimiz **X_train** ve **y_train**
    
    Test icin kullanacaklarimiz ise, **X_test** ve **y_test**â€™tir.
    
    burada yer alan **test_size** degeri ise, test setinin boyutunu belirtir. Burada verilerimizin yuzde 20â€™sini kullanacagini soyluyoruz.
    
    ### Lineer Regresyon Modeli
    
    Bagimli degiskenler ile bagimli degisken arasindaki lineer iliskiyi anlamak ve tahmin yapmak icin kullanilir. 
    
    **Lineer regresyon**, baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin doÄŸrusal bir kombinasyonunu kullanarak baÄŸÄ±mlÄ± deÄŸiÅŸkeni tahmin etmeye Ã§alÄ±ÅŸÄ±r. Ã–rneÄŸin, **ev fiyatlarÄ±nÄ±** tahmin etmek istiyorsanÄ±z ve evin bÃ¼yÃ¼klÃ¼ÄŸÃ¼, odalarÄ±n sayÄ±sÄ± gibi Ã¶zellikleri kullanÄ±yorsanÄ±z, **lineer regresyon** bu Ã¶zelliklerin aÄŸÄ±rlÄ±klÄ± toplamÄ±nÄ± kullanarak bir evin fiyatÄ±nÄ± tahmin etmeye Ã§alÄ±ÅŸÄ±r.
    
    ```python
    from sklearn.linear_model import LinearRegression
    ```
    
    **sklearn.linear_model** modÃ¼lÃ¼, farklÄ± **lineer modellerin ve regresyon** analizinin bulunduÄŸu bir kÃ¼tÃ¼phanedir.
    
    ---
    
    ```python
    model = LinearRegression() 
    # lineer regresyon modeli olusturuldu
    model.fit(X_train, y_train)
    # olusturulan modele, egitim icin hazirlanan setler ile egitildi.
    ```
    
    **LinearRegression()** fonksiyonu ile birlikte lineer regresyon modeli olusturup, **model** degiskenine atadik.
    
    **fit()** fonksiyonu ile de olusturdugumuz verisetlerini parametre olarak verdik.
    
    ***model.fit()*** iÅŸlemi tamamlandÄ±ÄŸÄ±nda, model artÄ±k **eÄŸitilmiÅŸ bir lineer regresyon modelini** temsil eder ve bu model, veriler Ã¼zerinde tahminler yapmak iÃ§in kullanÄ±labilir.
    
    ```python
    model.coef_
    # array([  37236.91280563,  -27364.78639333, -105348.03795271,
    #       150467.39394476,   -6918.61452005,  -10835.95507867])
    ```
    
    > **model.coef_**, eÄŸitilmiÅŸ bir lineer regresyon modelinin **katsayÄ±larÄ±nÄ± (coefficients)** dÃ¶ndÃ¼ren bir Ã¶zelliktir. Bu katsayÄ±lar, **baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin (Ã¶zelliklerin)** model Ã¼zerindeki **etkisini** temsil eder.
    > 
    > 
    > **Pozitif bir katsayÄ±**, o Ã¶zelliÄŸin **artan bir deÄŸere** sahip olduÄŸunda **baÄŸÄ±mlÄ± deÄŸiÅŸkenin** de artacaÄŸÄ±nÄ± gÃ¶sterirken, **negatif bir katsayÄ±** tam tersini ifade eder.
    > 
    
    ```python
    X.columns 
    # X veri setinin sutunlarinin adlarini verir.
    
    # Index(['median_income', '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY',
    #       'NEAR OCEAN'],
    #      dtype='object')
    ```
    
    ```python
    model.intercept_
    
    # Cikti: 110311.42503458945
    ```
    
    modelâ€™imizin **intercept_** ozelligine erisiyoruz. Bu ozellik, **dogrusal regresyon** modelimizin **kesim noktasi**â€™ini temsil eder.
    
    **Kesim noktasi**, bagimli degiskenin tahminindeki sabit terimi ifade eder. 
    
    Ã–rneÄŸin, ev fiyatlarÄ±nÄ± tahmin etmek istediÄŸinizi dÃ¼ÅŸÃ¼nelim. Evin bulunduÄŸu yer, odalarÄ±n sayÄ±sÄ±, gelir dÃ¼zeyi gibi Ã¶zellikleri kullanarak bir model oluÅŸturdunuz. Modelin kesme noktasÄ±, bu Ã¶zelliklerin tÃ¼mÃ¼ sÄ±fÄ±r olduÄŸunda, yani hiÃ§bir etkenin hesaba katÄ±lmadÄ±ÄŸÄ± bir durumda ev fiyatÄ±nÄ±n tahmin edilen baÅŸlangÄ±Ã§ deÄŸeridir.
    
    ---
    
    Simdi egittigimiz modelimizden cikti almamiz gerekiyor. Bagimsiz degiskenleri(odalarÄ±n sayÄ±sÄ±, metrekare, konum vb.)
    
    ```python
    y_test_predict = model.predict(X_test)
    ```
    
    > **model.predict()** fonksiyonu, eÄŸitilmiÅŸ bir makine Ã¶ÄŸrenimi modeli Ã¼zerinde **yeni veri noktalarÄ± veya veri setleri** iÃ§in tahmin yapmaya olanak saÄŸlar.
    > 
    > 
    > **model.predict(X_test)**, eÄŸitilmiÅŸ bir modelin test verileri Ã¼zerinde **tahmin** yapmasÄ±nÄ± saÄŸlar. Yani, bu kod satÄ±rÄ±, **eÄŸitilmiÅŸ bir modeli kullanarak test veri setindeki baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin (X_test)** deÄŸerlerine dayanarak baÄŸÄ±mlÄ± deÄŸiÅŸkenin (tahmin edilmek istenen deÄŸer) tahminlerini Ã¼retir.
    > 
    
    ```python
    from sklearn.metrics import r2_score
    r2_score(y_test, y_test_predict)
    
    # 0.5756744531463729
    ```
    
    > **r2_score**,
     R-kare olarak da bilinen belirli bir regresyon modelinin uyumunu 
    deÄŸerlendirmek iÃ§in kullanÄ±lan bir metrik veya Ã¶lÃ§Ã¼ttÃ¼r. Bu metrik, bir 
    regresyon modelinin veriye ne kadar iyi uyduÄŸunu, yani baÄŸÄ±mlÄ± 
    deÄŸiÅŸkenin (gerÃ§ek deÄŸerler) ne kadarÄ±nÄ± aÃ§Ä±kladÄ±ÄŸÄ±nÄ± Ã¶lÃ§er.
    > 
    > 
    > Verilen tahminler ve gerÃ§ek deÄŸerler arasÄ±ndaki iliÅŸkiyi deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r. EÄŸer tahmin edilen deÄŸerler **gerÃ§ek deÄŸerlere** daha yakÄ±nsa, **R-kare deÄŸeri 1'e** daha yakÄ±n olur. Ancak, eÄŸer tahminler gerÃ§ek deÄŸerlerden Ã§ok uzaksa veya deÄŸiÅŸkenlik Ã§ok yÃ¼ksekse, R-kare deÄŸeri dÃ¼ÅŸÃ¼k olur.
    > 
    
    ```python
    import plotly.express as px
    px.scatter(data_frame=house, x="longitude", y="latitude", 
               color="median_house_value", size=house.population / 100,
              hover_data=["ocean_proximity"], opacity=0.3)
    ```
    
    ![Untitled](1Regresyon%209b4f6322feb64b9e8591e410ddb86c73/Untitled%2011.png)

    # 2Classification

Siniflandirma, elimizdeki verileri belirli bir kategoriye alma islemidir. Ornegin bir e-postanin spam veya normal bir mail oldugunu belirlenmesinde kullanilir.

Veya bir goruntunun kopek mi kedi mi oldugunu belirlemede kullanilir.

Siniflandirma algoritmalarinda en cok kullanilanlar bunlardir:

SÄ±nÄ±flandÄ±rma algoritmalarÄ± arasÄ±nda en yaygÄ±n olanlarÄ± ÅŸunlardÄ±r:

- Naive Bayes
- Karar AÄŸaÃ§larÄ±
- K-En YakÄ±n KomÅŸu (KNN)
- Destek VektÃ¶r Makineleri (SVM)
- Lojistik Regresyon
- Yapay Sinir AÄŸlarÄ±

![df = pd.read_csv("../resources/customer_data.csv")](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled.png)

df = pd.read_csv("../resources/customer_data.csv")

![[df.info](http://df.info/)()](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%201.png)

[df.info](http://df.info/)()

> **fea_2** sutununda eksik veriler mevcut, diger sutunlar 1125 veri icerirken, fea_2 ise **976** veri iceriyor.
> 
> 
> float64 ve int64 DType ler olduguna gore, baskin tipler bunlar. Hic kategori verisinin olmadigini soyluyor.
> 

![df.describe()](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%202.png)

df.describe()

> **describe()** fonksyonu, sayisal sutunlarin istatistikel ozetini sunar.
> 
> 
> **Count:** GÃ¶zlemlenen veri sayÄ±sÄ±. Sutunda bulunan veri sayisini soyler. **fea_2** icin 1125
> 
> **Mean:** Ortalama deÄŸer
> 
> **Std:** Standart sapma, verilerin ne kadar daÄŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.
> 
> **Min:** En kÃ¼Ã§Ã¼k deÄŸer
> 
> **25%:** Alt Ã§eyrek (25. yÃ¼zdelik)
> 
> **50%:** Medyan (orta deÄŸer)
> 
> **75%:** Ãœst Ã§eyrek (75. yÃ¼zdelik)
> 
> **Max:** En bÃ¼yÃ¼k deÄŸer
> 
> Bu istatistikler, her bir sayÄ±sal sÃ¼tunun genel daÄŸÄ±lÄ±mÄ±nÄ± ve merkezi eÄŸilimini gÃ¶sterir. Ã–rneÄŸin, **fea_2** sÃ¼tununda eksik veri var (count deÄŸeri farklÄ±). DiÄŸer sayÄ±sal sÃ¼tunlardaki deÄŸerlerin daÄŸÄ±lÄ±mÄ± ve ortalamasÄ± da gÃ¶rÃ¼lebilir.
> 
> ???????????????????????
> 
> Verileri **kÃ¼Ã§Ã¼kten bÃ¼yÃ¼ÄŸe sÄ±raladÄ±ÄŸÄ±nÄ±zda**, **yÃ¼zde 25'lik dilim, tÃ¼m verilerin en dÃ¼ÅŸÃ¼k %25'ini**, yÃ¼zde 50'lik dilim (veya medyan) tÃ¼m verilerin orta deÄŸerini ve yÃ¼zde 75'lik dilim ise tÃ¼m verilerin en yÃ¼ksek %25'ini temsil eder. Bu, veri daÄŸÄ±lÄ±mÄ±nÄ±n genel bir gÃ¶rÃ¼nÃ¼mÃ¼nÃ¼ saÄŸlar. Ã–rneÄŸin, **fea_2** iÃ§in medyan deÄŸeri 1281.5'tir. Verilerin yarÄ±sÄ± bu deÄŸerden kÃ¼Ã§Ã¼k, yarÄ±sÄ± bu deÄŸerden bÃ¼yÃ¼ktÃ¼r demektir. Bu istatistikler, veri setindeki deÄŸerlerin nasÄ±l daÄŸÄ±ldÄ±ÄŸÄ±nÄ± anlamak iÃ§in kullanÄ±lÄ±r.
> 

![df.describe().T](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%203.png)

df.describe().T

Okunurlulugu arttirmak icin **Transpoz** aliyoruz.

![df.label](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%204.png)

df.label

```python
df.fea_2.mean(), df.fea_2.median()
# (1283.9113729508197, 1281.5)
```

> Bu iki deÄŸer, **"fea_2"** sÃ¼tunundaki verilerin merkezi eÄŸilimini anlamamÄ±za yardÄ±mcÄ± olur. Ã–rneÄŸin, burada ortalamaya yakÄ±n bir medyan deÄŸeri var, bu da bu sÃ¼tunun daÄŸÄ±lÄ±mÄ±nÄ±n ortalamaya yakÄ±n simetrik bir ÅŸekilde olduÄŸunu gÃ¶sterebilir.
> 

```python
df.fea_2.fillna(df.fea_2.mean(), inplace=True)
df.isna().sum()
```

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%205.png)

Bos degerleri bulunan **fea_2** sutununu, **fea_2** sutunun ortalamasiyla dolduruyoruz. **isna().sum()** metoduyla da o sutunda eksik veri kalip kalmadigini kontrol ediyoruz. Ciktiya gore herhangi bir eksik veri kalmamis.

```python
df.label.value_counts()

# 0  900
# 1  225
# Name: label, dtype: int64
```

**label** sutununda 2 deger var, **[0, 1]**

0 degerinden 900 tane, 1 degerinden ise 225 tane varmis. Goruldugu uzere sinifsal dagilimda buyuk **dengesizlik** var. Model egitimi sirasinda sorun cikartabilir. Cunku, **model egitimi** sirasinda verisi cok olan sinifa daha fazla **egilim** gosterir. Bu da modelimizin **yanilticiligini** gosterir.

### Matplotlib ile gorsellestir

```python
import seaborn as sns
import matplotlib.pyplot as plt

label_count = pd.DataFrame(
    {
        "Risk_Duzeyi":df.label.value_counts().index,
        "Toplam_Sayi":df.label.value_counts().values
    }
)

plt.title("Risk DaÄŸÄ±lÄ±m GrafiÄŸi")
#sns.barplot(data=label_count, x="Risk_Duzeyi", y="Toplam_Sayi", hue="Risk_Duzeyi")
sns.barplot(x=df.label.value_counts().index, 
            y=df.label.value_counts().values,hue=df.label.value_counts().index)
```

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%206.png)

> Bu kod, veri setinizdeki **"label"** sÃ¼tununda bulunan farklÄ± deÄŸerlerin **sayÄ±sÄ±nÄ± hesaplar**. Bu deÄŸerlerin her birini, **"Risk_Duzeyi"** olarak ve her bir deÄŸerin kaÃ§ kez geÃ§tiÄŸini, **"Toplam_Sayi"** olarak iÃ§eren bir veri Ã§erÃ§evesi oluÅŸturur. ArdÄ±ndan, bu verileri kullanarak **bir Ã§ubuk grafiÄŸi** oluÅŸturur.
> 
> 
> **hue=df.label.value_counts().index** olarak 
> belirtilmiÅŸ. Bu, label sÃ¼tunundaki farklÄ± deÄŸerleri temsil eder ve her 
> bir deÄŸer iÃ§in farklÄ± bir renk atanmasÄ±nÄ± saÄŸlar. Ã–rneÄŸin, 0 ve 1 gibi 
> iki farklÄ± etiket varsa, her bir etiket farklÄ± bir renkte 
> gÃ¶sterilebilir.
> 

### Verisetimizi egitime hazirlama

Tahminini yapacagimiz **label** sutununu **Bagimsiz Degiskenler**'i iceren verisetimizden cikartip yeni versetimizi olusturuyoruz.

![X = df.drop(â€labelâ€, axis=1)
X](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%207.png)

X = df.drop(â€labelâ€, axis=1)
X

**Bagimli degiskenimizi** de, **y** degiskenine atiyoruz

```python
X = df.drop("label", axis=1)
y = df.label
```

> **X = df.drop("label", axis=1):** Bu satÄ±r, **"label"** sÃ¼tununu **hedef deÄŸiÅŸken** olarak kabul edip, geri kalan **tÃ¼m sÃ¼tunlarÄ± (baÄŸÄ±msÄ±z deÄŸiÅŸkenleri)**
 iÃ§eren bir veri seti oluÅŸturur. drop fonksiyonu, "label" sÃ¼tununu 
(axis=1 ile sÃ¼tun olarak belirtilmiÅŸ) veri setinden Ã§Ä±karÄ±r ve geri 
kalan tÃ¼m sÃ¼tunlarÄ± X adlÄ± deÄŸiÅŸkene atar. Yani, X veri Ã§erÃ§evesi 
baÄŸÄ±msÄ±z deÄŸiÅŸkenleri iÃ§erir.
> 
> 
> **y = df.label:** Bu satÄ±r, **"label"** sÃ¼tununu (**bu durumda tahmin edilmek istenen deÄŸerler**) iÃ§eren veri setini **y** adlÄ± deÄŸiÅŸkene atar. Bu, genellikle makine Ã¶ÄŸrenimi modellerinde **tahmin edilmek istenen hedef deÄŸiÅŸkeni** temsil eder.
> 

![y](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%208.png)

y

### Verisetimizi egitim ve test olarak bolme

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                   random_state=42,
                                                   stratify=y)
```

Bu kod, veri setini **eÄŸitim** ve **test** alt kÃ¼melerine bÃ¶lmek iÃ§in kullanÄ±lÄ±r.

- **train_test_split()** fonksiyonu, veri setini **baÄŸÄ±msÄ±z deÄŸiÅŸkenler (X)** ve **hedef deÄŸiÅŸken (y)** olmak Ã¼zere iki farklÄ± kÄ±sma bÃ¶ler.
- **test_size=0.2** parametresi, veri setinin ne kadarÄ±nÄ±n **test** seti olarak ayrÄ±lacaÄŸÄ±nÄ± belirtir. Bu durumda, veri setinin **%20'si** test seti olarak ayrÄ±lacaktÄ±r.
- **random_state=42** parametresi, veri setini rastgele bÃ¶lerken kullanÄ±lacak olan rastgelelik durumunu belirler. Bu sayede iÅŸlemi tekrarladÄ±ÄŸÄ±nÄ±zda aynÄ± rastgele bÃ¶lme elde edilir, sonuÃ§larÄ±n
tekrarlanabilir olmasÄ±nÄ± saÄŸlar.
- **stratify=y** parametresi, veri setinin dengesiz sÄ±nÄ±flara sahip olmasÄ± durumunda sÄ±nÄ±f dengesizliÄŸini korumak iÃ§in kullanÄ±lÄ±r. Burada y (hedef deÄŸiÅŸken) tarafÄ±ndan belirtilen sÄ±nÄ±f
oranlarÄ±nÄ± eÄŸitim ve test setlerinde korur. **stratify** parametresi, genellikle **sÄ±nÄ±flandÄ±rma problemlerinde** kullanÄ±lÄ±r. Ã–zellikle **dengesiz sÄ±nÄ±flara sahip veri setlerinde**, sÄ±nÄ±f dengesizliÄŸini korumak iÃ§in oldukÃ§a Ã¶nemlidir.

SÄ±nÄ±f dengesizliÄŸi, bir sÄ±nÄ±fÄ±n diÄŸerlerine gÃ¶re **Ã§ok daha fazla temsil edilmesi** veya **az temsil edilmesi**
 durumudur. Ã–rneÄŸin, hastalÄ±k tespiti gibi durumlarda hastalÄ±klÄ± 
insanlar genellikle saÄŸlÄ±klÄ± insanlara gÃ¶re daha azdÄ±r. EÄŸer veri setindeki hastalÄ±klÄ± hastalar az temsil edilirse (Ã¶rneÄŸin, yalnÄ±zca %5), model hastalÄ±klÄ± olmayanlarÄ± Ã§ok iyi tahmin edebilirken, hastalÄ±klÄ± 
olanlarÄ± doÄŸru bir ÅŸekilde tahmin etmekte zorlanabilir.

Bu durumu Ã¶nlemek iÃ§in **stratify parametresi**, hedef deÄŸiÅŸkenin (genellikle y) sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± korur. **train_test_split** fonksiyonu, veri setini bÃ¶lmeden Ã¶nce her iki alt kÃ¼mede de orijinal veri setindeki sÄ±nÄ±f oranlarÄ±nÄ± koruyacak ÅŸekilde bÃ¶ler. Yani, eÄŸitim ve test setleri oluÅŸturulurken her iki alt kÃ¼mede de sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± oranÄ±, ana veri setiyle aynÄ± olur. Bu, modelin her iki veri setinde de eÄŸitilirken, her sÄ±nÄ±fÄ±n temsil edilme oranÄ±nÄ±n korunmasÄ±nÄ± saÄŸlar. Bu da modelin her sÄ±nÄ±f iÃ§in daha dengeli bir ÅŸekilde eÄŸitilmesine olanak tanÄ±r ve daha iyi genelleme yapmasÄ±na yardÄ±mcÄ± olabilir.

### RandomForestClassifier Modeli

<aside>
ğŸ¤– **RandomForestClassifier:** Birden Ã§ok karar aÄŸacÄ±nÄ± 
eÄŸiterek ve her bir aÄŸacÄ±n tahminlerini bir araya getirerek 
sÄ±nÄ±flandÄ±rma yapan bir ensemble (ensambl) modelidir. Her bir aÄŸaÃ§, 
rastgele Ã¶rneklemlerden ve rastgele Ã¶zelliklerden oluÅŸan alt veri 
setleri Ã¼zerinde eÄŸitilir. ArdÄ±ndan, bu aÄŸaÃ§larÄ±n tahminleri bir araya 
getirilerek en doÄŸru sonucun elde edilmesi hedeflenir.

</aside>

```python
from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)
```

> Bu kodda **RandomForestClassifier** sÄ±nÄ±fÄ±ndan bir model oluÅŸturup ardÄ±ndan bu modeli eÄŸittiniz. **RandomForestClassifier**, rastgele **orman algoritmasÄ±nÄ±** kullanan bir sÄ±nÄ±flandÄ±rma modelidir.
> 
> 
> **model_rf = RandomForestClassifier()** ile bir rastgele **orman modeli** oluÅŸturuldu ve daha sonra **model_rf.fit(X_train, y_train)** komutuyla bu model, **eÄŸitim veri seti (X_train ve y_train)** Ã¼zerinde eÄŸitildi. **fit fonksiyonu**, modelin **veriyi kullanarak iÃ§sel olarak parametrelerini ayarladÄ±ÄŸÄ±** ve v**eriyi temsil eden bir yapÄ± oluÅŸturduÄŸu aÅŸamadÄ±r**. EÄŸitim sÃ¼reci, modelin veriler arasÄ±ndaki iliÅŸkileri Ã¶ÄŸrenmesini saÄŸlar, bÃ¶ylece daha sonra **test veri seti** Ã¼zerinde doÄŸru tahminler yapabilir.
> 

### precision, recall, f1-score, support

```python
from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, model_rf.predict(X_test)))
```

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%209.png)

**classification_report**, doÄŸruluk, hassasiyet, duyarlÄ±lÄ±k (recall), F1 skoru ve destek (support) gibi deÄŸerleri iÃ§eren bir rapor sunar. **y_test**, gerÃ§ek etiketlerin olduÄŸu test seti verileri. **model_rf.predict(X_test)**, modelin test seti Ã¼zerindeki tahminlerini yapar.

**classification_report**

```
True Positive (TP): GerÃ§ek deÄŸeri pozitif olan ve model tarafÄ±ndan doÄŸru bir ÅŸekilde pozitif olarak tahmin edilen Ã¶rnekler.
False Positive (FP): GerÃ§ek deÄŸeri negatif olan ancak model tarafÄ±ndan yanlÄ±ÅŸ bir ÅŸekilde pozitif olarak tahmin edilen Ã¶rnekler.
True Negative (TN): GerÃ§ek deÄŸeri negatif olan ve model tarafÄ±ndan doÄŸru bir ÅŸekilde negatif olarak tahmin edilen Ã¶rnekler.
False Negative (FN): GerÃ§ek deÄŸeri pozitif olan ancak model tarafÄ±ndan yanlÄ±ÅŸ bir ÅŸekilde negatif olarak tahmin edilen Ã¶rnekler.

                    Predicted Positive    Predicted Negativie
Actual Positive           TP                     FN
Actual Negative           FP                     TN

```

**Hassasiyet (Precision):** Bu, modelin pozitif olarak tahmin ettiÄŸi Ã¶rneklerin gerÃ§ekten pozitif olanlarÄ±n oranÄ±nÄ± ifade eder. Yani, modelin doÄŸru pozitif tahminlerinin, tÃ¼m pozitif olarak tahmin edilen Ã¶rneklerin oranÄ±dÄ±r. FormÃ¼lÃ¼ ÅŸu ÅŸekildedir:

```
                TP
Precision = ----------
             TP + FP

```

**DuyarlÄ±lÄ±k (Recall):** Bu, gerÃ§ek pozitiflerin, modelin doÄŸru bir ÅŸekilde pozitif olarak tahmin ettiÄŸi Ã¶rneklerin oranÄ±nÄ± gÃ¶sterir. Yani, gerÃ§ekten pozitif olan Ã¶rneklerin, modelin tÃ¼m pozitif olarak tahmin ettiÄŸi Ã¶rneklerin iÃ§inde ne kadarÄ±nÄ±n doÄŸru bir ÅŸekilde tanÄ±mlandÄ±ÄŸÄ±nÄ± gÃ¶sterir. FormÃ¼lÃ¼ ÅŸu ÅŸekildedir

```
               TP
Recall = -------------
            TP + FN

```

**F1-score**, Ã¶zellikle dengesiz sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ±na sahip veri setlerinde model performansÄ±nÄ± deÄŸerlendirmek iÃ§in Ã¶nemli bir metriktir. Bu Ã¶lÃ§Ã¼m, precision ve recall arasÄ±ndaki dengeyi saÄŸlar. EÄŸer sÄ±nÄ±flar arasÄ±nda dengesizlik varsa (Ã¶rneÄŸin, bir sÄ±nÄ±f diÄŸerine gÃ¶re Ã§ok daha az Ã¶rneÄŸe sahipse), doÄŸruluk oranÄ± yetersiz bir performans gÃ¶sterebilir. F1-score, precision ve recall metriklerini birleÅŸtirerek, dengeli bir deÄŸerlendirme sunar. EÄŸer hem precision hem de recall yÃ¼ksekse, F1-score da yÃ¼ksek olacaktÄ±r. Ancak, bu metriklerden biri diÄŸerine gÃ¶re Ã§ok daha dÃ¼ÅŸÃ¼kse, 

F1-score da dÃ¼ÅŸÃ¼k olur. Bu nedenle, dengesiz veri setlerinde model performansÄ±nÄ±n daha iyi bir 
Ã¶lÃ§Ã¼tÃ¼ olarak kullanÄ±labilir.

```
            Precision x Recall
F1-Score = ----------------------
            Precision + Recall

```

**support**, her bir sÄ±nÄ±f iÃ§in veri setindeki gerÃ§ek Ã¶rnek sayÄ±sÄ±nÄ± temsil eder. FormÃ¼lÃ¼ yoktur, bu sadece her bir sÄ±nÄ±fÄ±n 
veri setindeki gÃ¶zlemlenen Ã¶rnek sayÄ±sÄ±nÄ± ifade eder. SÄ±nÄ±f dengesizliÄŸi durumunda, her bir sÄ±nÄ±fÄ±n support deÄŸeri farklÄ± olabilir. Ã–zellikle Ã§ok az Ã¶rneÄŸe sahip sÄ±nÄ±flar, modelin bu sÄ±nÄ±fÄ± Ã¶ÄŸrenme yeteneÄŸini etkileyebilir. Bu durumda, modelin daha az destekle karÅŸÄ±laÅŸtÄ±ÄŸÄ± sÄ±nÄ±flarÄ±n performansÄ± daha dÃ¼ÅŸÃ¼k olabilir. support metriÄŸi, her sÄ±nÄ±fÄ±n model performansÄ±nÄ± deÄŸerlendirirken gÃ¶z Ã¶nÃ¼nde bulundurulmasÄ±na yardÄ±mcÄ± olur.

**Macro Average:**
    SÄ±nÄ±f bazÄ±nda hesaplanan metriklerin (precision, recall, f1-score 
gibi) ortalamasÄ± alÄ±nÄ±r. Her sÄ±nÄ±fÄ±n metriÄŸi eÅŸit aÄŸÄ±rlÄ±kta 
deÄŸerlendirilir.

**Weighted Average:**
    SÄ±nÄ±f bazÄ±nda hesaplanan metriklerin (precision, recall, f1-score 
gibi) aÄŸÄ±rlÄ±klÄ± ortalamasÄ± alÄ±nÄ±r. Bu aÄŸÄ±rlÄ±klar, sÄ±nÄ±flarÄ±n veri 
setindeki Ã¶rnek sayÄ±larÄ±na gÃ¶re belirlenir. BÃ¼yÃ¼k sÄ±nÄ±flarÄ±n performansÄ±
 daha fazla etkiler.

Confision Matris

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2010.png)

### heatmap() fonksiyonu ile confusion_matrix cizme

```python
sns.heatmap(confusion_matrix(y_test, model_rf.predict(X_test)),
           annot=True, fmt=".1f")
```

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2011.png)

---

### DecisionTreeClassifier Modeli

<aside>
ğŸ¤– **DecisionTreeClassifier:** Temel bir sÄ±nÄ±flandÄ±rma 
algoritmasÄ±dÄ±r. Veri setini belirli koÅŸullara gÃ¶re bÃ¶lerek bir karar 
aÄŸacÄ± oluÅŸturur. Her bir dÃ¼ÄŸÃ¼m, en iyi bÃ¶lÃ¼nme noktasÄ±nÄ± bulmaya Ã§alÄ±ÅŸÄ±r
 ve veri setini bu Ã¶zelliklerin deÄŸerlerine gÃ¶re bÃ¶ler. BÃ¶ylece veriyi 
sÄ±nÄ±flandÄ±rmak iÃ§in bir dizi kararlar zinciri oluÅŸturur.

</aside>

Modelimizi olusturuyoruz.

```python
from sklearn.tree import DecisionTreeClassifier

model_tree = DecisionTreeClassifier()
model_tree.fit(X_train, y_train)

# DecisionTreeClassifier()
```

![print(classification_report(y_test, model_tree.predict(X_test)))](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2012.png)

print(classification_report(y_test, model_tree.predict(X_test)))

> Bu rapora gÃ¶re, modelin sÄ±nÄ±flandÄ±rma performansÄ± dÃ¼ÅŸÃ¼k olarak deÄŸerlendirilebilir. Precision deÄŸerleri sÄ±nÄ±f 0 iÃ§in 0.82, sÄ±nÄ±f 1 iÃ§in ise 0.26 olarak hesaplanmÄ±ÅŸtÄ±r. Bu, sÄ±nÄ±flandÄ±rÄ±lan Ã¶rneklerin sadece yaklaÅŸÄ±k olarak %26'sÄ±nÄ±n gerÃ§ek sÄ±nÄ±fÄ± 1 olan Ã¶rnekler olduÄŸunu gÃ¶sterir. Recall deÄŸerleri sÄ±nÄ±f 0 iÃ§in 0.78, sÄ±nÄ±f 1 iÃ§in ise 0.31 olarak hesaplanmÄ±ÅŸtÄ±r. Bu, gerÃ§ek sÄ±nÄ±fÄ± 1 olan Ã¶rneklerin sadece yaklaÅŸÄ±k olarak %31'inin doÄŸru bir ÅŸekilde sÄ±nÄ±flandÄ±rÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.
> 
> 
> F1-score deÄŸerleri sÄ±nÄ±f 0 iÃ§in 0.80, sÄ±nÄ±f 1 iÃ§in ise 0.28 olarak hesaplanmÄ±ÅŸtÄ±r. F1-score, precision ve recall metriklerinin harmonik ortalamasÄ±nÄ± temsil eder. Bu deÄŸerler, modelin sÄ±nÄ±flandÄ±rma performansÄ±nÄ±n dÃ¼ÅŸÃ¼k olduÄŸunu gÃ¶sterir.
> 
> Accuracy deÄŸeri 0.68 olarak hesaplanmÄ±ÅŸtÄ±r. Bu, tÃ¼m sÄ±nÄ±flarÄ±n doÄŸru bir ÅŸekilde sÄ±nÄ±flandÄ±rÄ±lma oranÄ±nÄ±n %68 olduÄŸunu gÃ¶sterir. Ancak, bu deÄŸer tek baÅŸÄ±na modelin performansÄ±nÄ± tam olarak yansÄ±tmayabilir, Ã§Ã¼nkÃ¼ sÄ±nÄ±flar arasÄ±ndaki dengesizlikleri gÃ¶z ardÄ± eder.
> 

### RandomOverSampler - Dengesizligi kaldirma

> **RandomOverSampler**, azÄ±nlÄ±k sÄ±nÄ±fÄ±ndaki Ã¶rneklerin sayÄ±sÄ±nÄ± Ã§oÄŸaltÄ±rken rastgele seÃ§im yapar. Bu seÃ§im iÅŸlemi, azÄ±nlÄ±k sÄ±nÄ±fÄ±ndaki Ã¶rneklerin kopyalarÄ±nÄ± alarak veri setini dengeler. Ã–rneÄŸin, azÄ±nlÄ±k sÄ±nÄ±fÄ±nda 100 Ã¶rnek varken, RandomOverSampler kullanÄ±ldÄ±ÄŸÄ±nda bu Ã¶rneklerin kopyalarÄ± alÄ±narak veri setinde daha fazla azÄ±nlÄ±k sÄ±nÄ±fÄ± Ã¶rneÄŸi oluÅŸturulabilir.
> 

```python
from imblearn.over_sampling import RandomOverSampler
```

```python
sampler = RandomOverSampler()
X_new, y_new = sampler.fit_resample(X, y)
```

> **fit_resample** fonksiyonu, Ã¶rnek sayÄ±sÄ±nÄ± artÄ±rmak veya azaltmak iÃ§in bir Ã¶rnekleme stratejisi uygular.
> 
> 
> Ã–ncelikle, **RandomOverSampler()** bir sampler nesnesi oluÅŸturur. Daha sonra **fit_resample**
>  yÃ¶ntemi, X ve y verilerini alarak, X_new ve y_new olarak yeniden 
> Ã¶rnekleme yapmaktadÄ±r. Bu iÅŸlem, orijinal veri kÃ¼mesindeki sÄ±nÄ±f 
> dengesizliÄŸini ele almak iÃ§in azÄ±nlÄ±k sÄ±nÄ±fÄ±ndaki Ã¶rnekleri artÄ±rÄ±rken, 
> Ã§oÄŸunluk sÄ±nÄ±fÄ±ndaki Ã¶rnekleri azaltÄ±r veya deÄŸiÅŸtirir.
> 

![y_new.value_counts()](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2013.png)

y_new.value_counts()

### Verisetlerini Hazirlama

```python
X_newtr, X_newts, y_newtr, y_newts = train_test_split(X_new, y_new, test_size=0.2,
                                                   random_state=42)
```

> **X_new** ve **y_new** veri setlerini train ve test setlerine bÃ¶lmek iÃ§in **train_test_split** iÅŸlevini kullanÄ±r. **test_size=0.2** parametresi, veri setinin **%20**'sini
 test seti olarak ayarlar. random_state=42 parametresi ise bÃ¶lme 
iÅŸleminin rastgele yapÄ±lmasÄ±nÄ± saÄŸlar, bu sayede her Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda 
aynÄ± rastgele bÃ¶lme elde edilir. BÃ¶ylece eÄŸitim ve test verileri 
rastgele ve dengelenmiÅŸ ÅŸekilde ayrÄ±lÄ±r.
> 

### Modeli Egitme

```python
model_rfn = RandomForestClassifier()
model_rfn.fit(X_newtr, y_newtr)

# RandomForestClassifier()
```

![print(classification_report(y_newts, model_rfn.predict(X_newts)))](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2014.png)

print(classification_report(y_newts, model_rfn.predict(X_newts)))

> **Precision**: DoÄŸru pozitif tahminlerin toplam pozitif tahminlere oranÄ±dÄ±r. 0 sÄ±nÄ±fÄ± iÃ§in %93, 1 sÄ±nÄ±fÄ± iÃ§in %88.
**Recall**: GerÃ§ek pozitiflerin doÄŸru bir ÅŸekilde tahmin edilme oranÄ±dÄ±r. 0 sÄ±nÄ±fÄ± iÃ§in %86, 1 sÄ±nÄ±fÄ± iÃ§in %93.
**F1-Score**: Precision ve recall metriklerinin harmonik ortalamasÄ±dÄ±r. Denge metriÄŸi olarak kullanÄ±lÄ±r. 0 sÄ±nÄ±fÄ± iÃ§in %89, 1 sÄ±nÄ±fÄ± iÃ§in %91.
**Accuracy**: DoÄŸru tahminlerin toplam tahminlere oranÄ±dÄ±r. %90.
**Macro Avg**: SÄ±nÄ±flar arasÄ±nda aÄŸÄ±rlÄ±klandÄ±rÄ±lmamÄ±ÅŸ ortalamadÄ±r. SÄ±nÄ±f sayÄ±sÄ±na baÄŸlÄ± olarak dengelenmemiÅŸ bir Ã¶lÃ§Ã¼dÃ¼r.
**Weighted Avg**: Her sÄ±nÄ±fÄ±n desteÄŸine (support) gÃ¶re aÄŸÄ±rlÄ±klÄ± ortalamadÄ±r. SÄ±nÄ±flar arasÄ±ndaki dengesizliÄŸi gÃ¶z Ã¶nÃ¼nde bulundurur.
> 
> 
> Bu sonuÃ§lar, modelin genel olarak iyi performans gÃ¶sterdiÄŸini ve sÄ±nÄ±flar arasÄ±nda dengeli bir tahmin yapabildiÄŸini gÃ¶steriyor.
> 

```python
sns.heatmap(confusion_matrix(y_newts, model_rfn.predict(X_newts)),
           annot=True, fmt=".1f")
```

![Untitled](2Classification%2076283d456598455c8d5ed862ab46cf31/Untitled%2015.png)

# 3Cluster

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled.png)

Siniflandirma problemlerinde, elimizdeki verileri bir etiket atayip, onu hangi veri oldugunu dogrudan soyleyebiliyoruz. Bunu (b) seklinde rahatca gorebiliriz. Ama **Cluster** bolumune gectigimiz zaman ise elimizdeki sonuc verilerini herhangi bir sonuca baglayamiyoruz. Sadece verileri belirli gruplara bolup oraya ait olduklarini soyluyoruz.

- Musteri Segmentasyonu
- Pazar Segmentasyonu
- Saglik ve Goruntu Isleme

**Cluster** benzer ozellikteki verileri bir araya getirerek gruplar olustururuz. **Unsupervised(gozetimsiz)** ogrenme yontemidir. Veri setindeki Ã¶rneklerin birbirine olan benzerliklerini Ã¶lÃ§erek, bu Ã¶rnekleri gruplara ayÄ±rÄ±r. Bu gruplar, veri setindeki yapÄ±yÄ± ve iliÅŸkileri anlamamÄ±za yardÄ±mcÄ± olur.

Cluster analizi icin kullanilan en yaygin yontem ise **K-Means** algoritmasidir. 

Veri setindeki ornekler belirli kumelere bolunur. K-Means ise onceden belirlenen belirli sayida kumeye sahip olacagimizi varsayar, her bir kume icin merkez noktasi secer. Daha sonra ise her bir ornegi en yakin merkeze atar. Bu islemleri tekrar ederek optimum bir kume merkezi guncellemis olur. 

Clusterâ€™in basari kriteri; kume elemanlarini, kume merkezine olan uzakligini minimum seviyede tutmak, Kumeler arasi mesafeyi ise maximum tutmaktir.

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%201.png)

```python
import pandas as pd
df = pd.read_csv("../resources/Mall_Customers.csv")
df
```

> Veri seti, bir alÄ±ÅŸveriÅŸ merkezindeki mÃ¼ÅŸterilere iliÅŸkin bilgileri iÃ§ermektedir. DataFrame'in sÃ¼tunlarÄ±, "CustomerID" (mÃ¼ÅŸteri kimliÄŸi), "Gender" (cinsiyet), "Age" (yaÅŸ), "Annual Income" (yÄ±llÄ±k gelir) ve "Spending Score" (harcama puanÄ±) olarak adlandÄ±rÄ±lmÄ±ÅŸtÄ±r.
> 

### Sacilim grafiginin cizimi

```python
import seaborn as sns
sns.scatterplot(data=df, x="Age", y="Annual Income (k$)",
               hue="Gender", alpha=0.5)
```

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%202.png)

> Bu saÃ§Ä±lma grafiÄŸi, mÃ¼ÅŸterilerin **yaÅŸlarÄ±** ile **yÄ±llÄ±k gelirlerini** cinsiyete gÃ¶re gÃ¶steriyor gibi gÃ¶rÃ¼nÃ¼yor.
> 
> 
> mÃ¼ÅŸterilerin yaÅŸlarÄ± ve yÄ±llÄ±k gelirleri arasÄ±ndaki iliÅŸkiyi cinsiyet Ã¼zerinden gÃ¶rselleÅŸtiriyor.
> 

```python
sns.scatterplot(data=df, x="Age", y="Spending Score (1-100)",
               hue="Gender", alpha=0.5)
```

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%203.png)

> **YaÅŸ** ile **harcama puanlarÄ±** arasÄ±ndaki iliÅŸkiyi cinsiyete gÃ¶re gÃ¶steriyor gibi gÃ¶rÃ¼nÃ¼yor.
> 
> 
> Harcama
>  puanlarÄ±, yaÅŸa gÃ¶re nasÄ±l daÄŸÄ±lÄ±yor, hangi yaÅŸ grubundaki mÃ¼ÅŸterilerin 
> daha yÃ¼ksek veya dÃ¼ÅŸÃ¼k harcama puanÄ±na sahip olduÄŸu gibi konularda fikir
>  verebilir.
> 

### Verisetinin hazirlanmasi

```python
X = df[["Age", "Spending Score (1-100)"]]
X
```

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%204.png)

### Modeli olusturma

```python
from sklearn.cluster import KMeans
uzaklik = []
for n in range(1 , 11):
    model = KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 42)
    model.fit(X)
    uzaklik.append(model.inertia_)
```

> Bu kod, **sklearn** kÃ¼tÃ¼phanesindeki **KMeans** algoritmasÄ±nÄ± kullanarak farklÄ± kÃ¼me sayÄ±larÄ± iÃ§in **KMeans** modeli oluÅŸturuyor. **uzaklik** adÄ±nda bir boÅŸ liste oluÅŸturuluyor. Daha sonra **range(1, 11)** dÃ¶ngÃ¼sÃ¼ ile kÃ¼me sayÄ±sÄ±nÄ± 1'den 10'a kadar artarak deÄŸiÅŸtiriyoruz. Her iterasyonda, belirtilen kÃ¼me sayÄ±sÄ± (n_clusters) ve diÄŸer parametrelerle birlikte (init, n_init, max_iter, tol, random_state) **K-Means** modeli oluÅŸturuluyor. Bu modeller **fit()** metodu ile veri setine(X) uyduruluyor.Modelin **inertia deÄŸeri (model.inertia_)** hesaplanÄ±p, bu deÄŸer uzaklik listesine ekleniyor. Bu deÄŸer, KMeans algoritmasÄ±nÄ±n o kÃ¼me sayÄ±sÄ± iÃ§in inertia deÄŸerini temsil eder.SonuÃ§ olarak, bu kod farklÄ± kÃ¼me sayÄ±larÄ± iÃ§in **KMeans** modeli **oluÅŸturup**, her bir kÃ¼me sayÄ±sÄ± iÃ§in **inertia** deÄŸerlerini hesaplayarak bu deÄŸerleri uzaklik listesine **kaydeder**. Bu deÄŸerler, kÃ¼me sayÄ±sÄ±nÄ± belirlemede yardÄ±mcÄ± olabilir, optimal kÃ¼me sayÄ±sÄ±nÄ± belirlemek iÃ§in inertia deÄŸerlerinin grafiÄŸi incelenebilir. Kod farklÄ± kÃ¼me sayÄ±larÄ± iÃ§in **KMeans algoritmasÄ±nÄ±** kullanarak her bir kÃ¼me sayÄ±sÄ±nÄ±n **"inertia_"** deÄŸerini hesaplar. Bu deÄŸer, **veri noktalarÄ±nÄ±n kÃ¼me merkezine** olan karesel uzaklÄ±klarÄ±nÄ±n toplamÄ±nÄ± ifade eder. Azalan **inertia deÄŸeri**, veri noktalarÄ±nÄ±n kÃ¼meler iÃ§inde daha yoÄŸun ve merkezlere daha yakÄ±n olduÄŸunu gÃ¶sterir. **"uzaklik"** listesi, farklÄ± kÃ¼me sayÄ±larÄ± iÃ§in inertia deÄŸerlerini iÃ§erir ve genellikle en uygun kÃ¼me sayÄ±sÄ±nÄ± belirlemek iÃ§in **"dirsek"** yÃ¶ntemiyle kullanÄ±lÄ±r. Bu yÃ¶ntem, veri setinin homojenliÄŸini ve kÃ¼meler arasÄ±ndaki farklÄ±lÄ±klarÄ± anlamamÄ±za yardÄ±mcÄ± olur. 
Ä°nertia, KMeans algoritmasÄ± tarafÄ±ndan belirlenen kÃ¼me merkezlerinin, her veri noktasÄ±nÄ±n merkeze olan uzaklÄ±klarÄ±nÄ±n karelerinin toplamÄ±nÄ± ifade eder. Bu, her bir veri noktasÄ±nÄ±n kendi kÃ¼me merkezine olan uzaklÄ±ÄŸÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼dÃ¼r ve bu uzaklÄ±klarÄ±n kareleri toplamÄ±dÄ±r. UzaklÄ±k ise genel olarak bir noktanÄ±n diÄŸer bir noktaya olan mesafesini belirtir. Ä°nertia, KMeans algoritmasÄ±nÄ±n kÃ¼meleme performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±rken, uzaklÄ±k genellikle iki nokta arasÄ±ndaki mesafeyi belirtmek iÃ§in kullanÄ±lÄ±r. KÄ±sacasÄ±, inertia, kÃ¼me merkezlerine olan toplam uzaklÄ±k miktarÄ±nÄ± temsil ederken, uzaklÄ±k ise genellikle iki nokta arasÄ±ndaki mesafeyi belirtir.
> 

### Dirsek cizimi

```python
import matplotlib.pyplot as plt
import numpy as np
plt.figure(1 , figsize = (15 ,6))
plt.plot(np.arange(1 , 11) , uzaklik , 'o')
plt.plot(np.arange(1 , 11) , uzaklik , '-' , alpha = 0.5)
plt.xlabel('KÃ¼me SayÄ±sÄ±') , plt.ylabel('UzaklÄ±k')
plt.show()
```

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%205.png)

> 
> 
> 
> Bu kod, kÃ¼me sayÄ±sÄ±nÄ±n artÄ±ÅŸÄ±yla **inertia** deÄŸerlerinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lÄ±r.
> 
> **matplotlib** kÃ¼tÃ¼phanesi kullanÄ±larak bir grafik oluÅŸturuluyor. **figure()** fonksiyonu bir figÃ¼r oluÅŸturur ve figsize parametresiyle figÃ¼rÃ¼n boyutlarÄ± belirlenir.
> 
> **plt.plot()** fonksiyonu, kÃ¼me sayÄ±sÄ±nÄ±n deÄŸiÅŸimi ile **inertia** deÄŸerlerinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶stermek iÃ§in kullanÄ±lÄ±r. **np.arange(1, 11)** ile **x-ekseni (kÃ¼me sayÄ±sÄ±)** deÄŸerleri belirlenirken, uzaklik listesindeki inertia deÄŸerleri **y-ekseni (uzaklÄ±k)** olarak atanÄ±r. **'o'** parametresiyle nokta grafikleri, **'-'** parametresiyle ise Ã§izgi grafiÄŸi Ã§izilir. **alpha=0.5** ise Ã§izgilerin saydamlÄ±ÄŸÄ±nÄ± ayarlar.
> 
> **plt.xlabel()** ve **plt.ylabel()** ile x ve y eksenlerine etiketler atanÄ±r.
> 
> Son olarak, **plt.show()** ile grafiÄŸin gÃ¶rÃ¼ntÃ¼lenmesi saÄŸlanÄ±r.
> 
> Bu grafik, kÃ¼me sayÄ±sÄ±nÄ±n artÄ±ÅŸÄ±yla **inertia** deÄŸerlerinin nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶stererek, optimal kÃ¼me sayÄ±sÄ±nÄ± belirlemede yardÄ±mcÄ± olabilir. Genellikle **"dirsek yÃ¶ntemi"** olarak adlandÄ±rÄ±lan yaklaÅŸÄ±m, grafikte dirsek ÅŸeklinde bir kÄ±rÄ±lma noktasÄ±nÄ±n olduÄŸu yerin optimal kÃ¼me sayÄ±sÄ± olabileceÄŸini gÃ¶sterir.
> 

### Model egitimi

```python
model = KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, 
                        tol=0.0001,  random_state= 42 , algorithm='elkan') 
model.fit(X)
new_labels = model.labels_
merkezler = model.cluster_centers_
```

```
n_clusters = 4: OluÅŸturulacak kÃ¼me sayÄ±sÄ±dÄ±r. Burada 4 kÃ¼me belirlenmiÅŸtir.

init='k-means++': BaÅŸlangÄ±Ã§ merkezlerinin 'k-means++' yÃ¶ntemiyle belirlenmesini saÄŸlar. Bu yÃ¶ntem, daha iyi baÅŸlangÄ±Ã§ merkezi konumlarÄ± elde etmeye Ã§alÄ±ÅŸÄ±r.

n_init = 10: Rastgele baÅŸlangÄ±Ã§ merkezleri seÃ§imi iÃ§in tekrar sayÄ±sÄ±dÄ±r. Bu durumda, 10 farklÄ± rastgele baÅŸlangÄ±Ã§ seÃ§imi yapÄ±lÄ±r ve en iyi sonuÃ§larÄ± veren baÅŸlangÄ±Ã§lar seÃ§ilir.

max_iter=300: Her bir tekrar iÃ§in maksimum iterasyon sayÄ±sÄ±dÄ±r. Algoritma, maksimum iterasyon sayÄ±sÄ±na ulaÅŸana veya kÃ¼meleme konverge edene kadar Ã§alÄ±ÅŸÄ±r.

tol=0.0001: KÃ¼melerin deÄŸiÅŸim miktarÄ± (tolerans) bu deÄŸerin altÄ±na dÃ¼ÅŸtÃ¼ÄŸÃ¼nde algoritmanÄ±n sonlanmasÄ±nÄ± saÄŸlar.

random_state= 42: Rastgele sayÄ± Ã¼retimi iÃ§in seed deÄŸeridir. Bu sayede algoritmanÄ±n tekrar Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± durumunda aynÄ± sonuÃ§larÄ± alabiliriz.

algorithm='elkan': KullanÄ±lacak algoritma tÃ¼rÃ¼dÃ¼r. 'elkan', KMeans algoritmasÄ±nÄ±n hÄ±zlandÄ±rÄ±lmÄ±ÅŸ bir versiyonudur.
```

> model.fit(X): Modelin verilerle eÄŸitilmesini saÄŸlar. Veri seti X ile eÄŸitilir.
> 

> new_labels = model.labels_: Her bir veri noktasÄ±nÄ±n, k-means algoritmasÄ± tarafÄ±ndan atanan etiketlerini iÃ§eren labels_ Ã¶zelliÄŸi kullanÄ±larak yeni etiketler oluÅŸturulur.
> 

> merkezler = model.cluster_centers_: KÃ¼me merkezlerini iÃ§eren cluster_centers_ Ã¶zelliÄŸi kullanÄ±larak, her kÃ¼menin merkezi bulunur.
> 

```python
new_labels
```

![Untitled](3Cluster%20d6559f0b493e4cd5b0ff4a1f4d8aa515/Untitled%206.png)

> Bu Ã§Ä±ktÄ±, her bir veri noktasÄ±nÄ±n hangi kÃ¼meye atandÄ±ÄŸÄ±nÄ± gÃ¶steren etiketleri iÃ§erir. Her bir etiket, o veri noktasÄ±nÄ±n hangi kÃ¼meye ait olduÄŸunu belirtir. Ã–rneÄŸin, new_labels dizisinin ilk elemanÄ± 3 ise, ilk veri noktasÄ±nÄ±n 4. kÃ¼meye ait olduÄŸunu gÃ¶sterir. Bu etiketler, KMeans algoritmasÄ± tarafÄ±ndan belirlenen kÃ¼meleri temsil eder. Etiketler, her bir veri noktasÄ±nÄ± ilgili kÃ¼melere atayarak kÃ¼meleme iÅŸlemini gerÃ§ekleÅŸtirir.
>#
